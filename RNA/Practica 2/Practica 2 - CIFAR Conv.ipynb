{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practica 2 - CIFAR Conv.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QR5PU41pQiUh","colab_type":"code","outputId":"81046693-7863-4858-f37e-0216a062f8a3","executionInfo":{"status":"ok","timestamp":1579632086630,"user_tz":-60,"elapsed":4630583,"user":{"displayName":"Juan LÃ³pez","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgUzCzE30s7_4IkyEOfNxGyZPTu_aDNuvfoOK0dw=s64","userId":"04861049710763598966"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from __future__ import print_function\n","import keras\n","from keras.datasets import cifar10\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers.normalization import BatchNormalization as BN\n","from keras.layers import GaussianNoise as GN\n","from keras.optimizers import SGD\n","from keras.callbacks import LearningRateScheduler as LRS\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 100\n","num_classes = 10\n","epochs = 150\n","\n","\n","#### LOAD AND TRANSFORM\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","x_train /= 255\n","x_test /= 255\n","\n","print(x_train.shape)\n","print(x_test.shape)\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","#### DATA AUGMENTATION\n","datagen = ImageDataGenerator(\n","            featurewise_center=True,\n","            featurewise_std_normalization=True,\n","            width_shift_range=0.2,\n","            height_shift_range=0.2,\n","            rotation_range=20,\n","            zoom_range=[1.0,1.2],\n","            horizontal_flip=True)\n","\n","datagen.fit(x_train)\n","\n","testdatagen = ImageDataGenerator(\n","    featurewise_center=True,\n","    featurewise_std_normalization=True,\n",")\n","\n","testdatagen.fit(x_train)\n","\n","## DEF A BLOCK CONV + BN + GN + MAXPOOL\n","def CBGN(model,filters,ishape=0):\n","  if (ishape!=0):\n","    model.add(Conv2D(filters, (3, 3), padding='same',\n","                 input_shape=ishape))\n","  else:\n","    model.add(Conv2D(filters, (3, 3), padding='same'))\n","  \n","  model.add(BN())\n","  model.add(GN(0.3))\n","  model.add(Activation('relu'))\n","\n","  model.add(Conv2D(filters, (3, 3), padding='same'))\n","  model.add(BN())\n","  model.add(GN(0.3))\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  \n","  \n","  return model\n","\n","  \n","## DEF NN TOPOLOGY  \n","model = Sequential()\n","\n","model=CBGN(model,32,x_train.shape[1:])\n","model=CBGN(model,64)\n","model=CBGN(model,128)\n","model=CBGN(model,256)\n","model=CBGN(model,512)\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))\n","\n","\n","model.summary()\n","\n","#### LEARNING RATE SCHEDULER\n","def scheduler(epoch):\n","  if epoch < 75:\n","    return .1\n","  elif epoch < 125:\n","    return 0.01\n","  else:\n","    return 0.001\n","\n","set_lr = LRS(scheduler)\n","\n","## OPTIM AND COMPILE\n","opt = SGD(lr=0.1, decay=1e-6)\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","## TRAINING\n","model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","          steps_per_epoch=len(x_train) / batch_size, \n","          epochs=epochs,\n","          validation_data=testdatagen.flow(x_test, y_test),\n","          callbacks=[set_lr],\n","          shuffle=True)\n","\n","## TEST\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","#print('Test loss:', scores[0])\n","#print('Test accuracy:', scores[1])"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n","(50000, 32, 32, 3)\n","(10000, 32, 32, 3)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","gaussian_noise_1 (GaussianNo (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","gaussian_noise_2 (GaussianNo (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","gaussian_noise_3 (GaussianNo (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","gaussian_noise_4 (GaussianNo (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","gaussian_noise_5 (GaussianNo (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","gaussian_noise_6 (GaussianNo (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 4, 4, 256)         295168    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n","_________________________________________________________________\n","gaussian_noise_7 (GaussianNo (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 4, 4, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n","_________________________________________________________________\n","gaussian_noise_8 (GaussianNo (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 2, 2, 512)         1180160   \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 2, 2, 512)         2048      \n","_________________________________________________________________\n","gaussian_noise_9 (GaussianNo (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 2, 2, 512)         2048      \n","_________________________________________________________________\n","gaussian_noise_10 (GaussianN (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","activation_10 (Activation)   (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","activation_11 (Activation)   (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                5130      \n","_________________________________________________________________\n","activation_12 (Activation)   (None, 10)                0         \n","=================================================================\n","Total params: 4,987,946\n","Trainable params: 4,983,978\n","Non-trainable params: 3,968\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/150\n","500/500 [==============================] - 42s 83ms/step - loss: 1.8613 - acc: 0.3208 - val_loss: 1.6823 - val_acc: 0.4221\n","Epoch 2/150\n","500/500 [==============================] - 30s 61ms/step - loss: 1.3880 - acc: 0.4929 - val_loss: 1.2074 - val_acc: 0.5602\n","Epoch 3/150\n","500/500 [==============================] - 31s 62ms/step - loss: 1.1798 - acc: 0.5746 - val_loss: 1.0553 - val_acc: 0.6360\n","Epoch 4/150\n","500/500 [==============================] - 31s 61ms/step - loss: 1.0403 - acc: 0.6287 - val_loss: 1.1760 - val_acc: 0.6331\n","Epoch 5/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.9399 - acc: 0.6679 - val_loss: 1.0016 - val_acc: 0.6759\n","Epoch 6/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.8596 - acc: 0.6993 - val_loss: 0.7741 - val_acc: 0.7459\n","Epoch 7/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.8082 - acc: 0.7180 - val_loss: 0.7707 - val_acc: 0.7378\n","Epoch 8/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.7674 - acc: 0.7331 - val_loss: 0.8110 - val_acc: 0.7402\n","Epoch 9/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.7273 - acc: 0.7449 - val_loss: 0.7200 - val_acc: 0.7598\n","Epoch 10/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.6977 - acc: 0.7566 - val_loss: 0.7382 - val_acc: 0.7497\n","Epoch 11/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.6699 - acc: 0.7677 - val_loss: 0.7159 - val_acc: 0.7696\n","Epoch 12/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.6522 - acc: 0.7741 - val_loss: 0.6475 - val_acc: 0.7849\n","Epoch 13/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.6295 - acc: 0.7799 - val_loss: 0.8312 - val_acc: 0.7335\n","Epoch 14/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.6048 - acc: 0.7895 - val_loss: 0.6273 - val_acc: 0.7903\n","Epoch 15/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.5934 - acc: 0.7942 - val_loss: 1.1029 - val_acc: 0.6959\n","Epoch 16/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.5708 - acc: 0.8016 - val_loss: 0.7012 - val_acc: 0.7779\n","Epoch 17/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.5582 - acc: 0.8055 - val_loss: 0.5383 - val_acc: 0.8245\n","Epoch 18/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.5503 - acc: 0.8092 - val_loss: 0.5751 - val_acc: 0.8156\n","Epoch 19/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.5298 - acc: 0.8162 - val_loss: 0.5558 - val_acc: 0.8177\n","Epoch 20/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.5201 - acc: 0.8217 - val_loss: 0.5462 - val_acc: 0.8236\n","Epoch 21/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.5098 - acc: 0.8236 - val_loss: 0.5574 - val_acc: 0.8165\n","Epoch 22/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.5006 - acc: 0.8269 - val_loss: 0.5368 - val_acc: 0.8219\n","Epoch 23/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.4795 - acc: 0.8304 - val_loss: 0.4877 - val_acc: 0.8407\n","Epoch 24/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.4788 - acc: 0.8332 - val_loss: 0.5345 - val_acc: 0.8288\n","Epoch 25/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.4722 - acc: 0.8341 - val_loss: 0.4868 - val_acc: 0.8404\n","Epoch 26/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.4598 - acc: 0.8417 - val_loss: 0.4946 - val_acc: 0.8376\n","Epoch 27/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.4499 - acc: 0.8419 - val_loss: 0.5327 - val_acc: 0.8306\n","Epoch 28/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.4434 - acc: 0.8468 - val_loss: 0.4415 - val_acc: 0.8551\n","Epoch 29/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.4372 - acc: 0.8478 - val_loss: 0.4091 - val_acc: 0.8653\n","Epoch 30/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.4333 - acc: 0.8488 - val_loss: 0.4832 - val_acc: 0.8498\n","Epoch 31/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.4191 - acc: 0.8547 - val_loss: 0.4616 - val_acc: 0.8443\n","Epoch 32/150\n","500/500 [==============================] - 29s 59ms/step - loss: 0.4088 - acc: 0.8574 - val_loss: 0.4477 - val_acc: 0.8575\n","Epoch 33/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.4090 - acc: 0.8567 - val_loss: 0.4552 - val_acc: 0.8511\n","Epoch 34/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.4049 - acc: 0.8591 - val_loss: 0.4569 - val_acc: 0.8539\n","Epoch 35/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3965 - acc: 0.8619 - val_loss: 0.5226 - val_acc: 0.8397\n","Epoch 36/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3930 - acc: 0.8636 - val_loss: 0.3920 - val_acc: 0.8719\n","Epoch 37/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3793 - acc: 0.8675 - val_loss: 0.4195 - val_acc: 0.8626\n","Epoch 38/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3837 - acc: 0.8677 - val_loss: 0.4544 - val_acc: 0.8548\n","Epoch 39/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3740 - acc: 0.8692 - val_loss: 0.4601 - val_acc: 0.8563\n","Epoch 40/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3696 - acc: 0.8710 - val_loss: 0.4680 - val_acc: 0.8547\n","Epoch 41/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3651 - acc: 0.8715 - val_loss: 0.4099 - val_acc: 0.8638\n","Epoch 42/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3593 - acc: 0.8752 - val_loss: 0.3813 - val_acc: 0.8740\n","Epoch 43/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3542 - acc: 0.8753 - val_loss: 0.3928 - val_acc: 0.8751\n","Epoch 44/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.3525 - acc: 0.8766 - val_loss: 0.4574 - val_acc: 0.8596\n","Epoch 45/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3450 - acc: 0.8795 - val_loss: 0.4608 - val_acc: 0.8592\n","Epoch 46/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3439 - acc: 0.8783 - val_loss: 0.3958 - val_acc: 0.8734\n","Epoch 47/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3401 - acc: 0.8801 - val_loss: 0.4228 - val_acc: 0.8643\n","Epoch 48/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3345 - acc: 0.8826 - val_loss: 0.4636 - val_acc: 0.8573\n","Epoch 49/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3285 - acc: 0.8845 - val_loss: 0.3949 - val_acc: 0.8728\n","Epoch 50/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3248 - acc: 0.8863 - val_loss: 0.4757 - val_acc: 0.8613\n","Epoch 51/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.3233 - acc: 0.8862 - val_loss: 0.4436 - val_acc: 0.8641\n","Epoch 52/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3163 - acc: 0.8886 - val_loss: 0.3711 - val_acc: 0.8844\n","Epoch 53/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3188 - acc: 0.8885 - val_loss: 0.4352 - val_acc: 0.8637\n","Epoch 54/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3131 - acc: 0.8898 - val_loss: 0.3937 - val_acc: 0.8795\n","Epoch 55/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3106 - acc: 0.8912 - val_loss: 0.3743 - val_acc: 0.8797\n","Epoch 56/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.3063 - acc: 0.8926 - val_loss: 0.3322 - val_acc: 0.8915\n","Epoch 57/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.3019 - acc: 0.8932 - val_loss: 0.4285 - val_acc: 0.8661\n","Epoch 58/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2981 - acc: 0.8952 - val_loss: 0.3978 - val_acc: 0.8738\n","Epoch 59/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.2930 - acc: 0.8968 - val_loss: 0.3834 - val_acc: 0.8779\n","Epoch 60/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2946 - acc: 0.8971 - val_loss: 0.3748 - val_acc: 0.8830\n","Epoch 61/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2902 - acc: 0.8975 - val_loss: 0.3779 - val_acc: 0.8803\n","Epoch 62/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2861 - acc: 0.9002 - val_loss: 0.3935 - val_acc: 0.8693\n","Epoch 63/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2811 - acc: 0.9013 - val_loss: 0.3573 - val_acc: 0.8841\n","Epoch 64/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.2848 - acc: 0.8995 - val_loss: 0.3907 - val_acc: 0.8808\n","Epoch 65/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2743 - acc: 0.9042 - val_loss: 0.3715 - val_acc: 0.8842\n","Epoch 66/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.2758 - acc: 0.9040 - val_loss: 0.3838 - val_acc: 0.8810\n","Epoch 67/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.2731 - acc: 0.9042 - val_loss: 0.3708 - val_acc: 0.8834\n","Epoch 68/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2718 - acc: 0.9043 - val_loss: 0.4014 - val_acc: 0.8805\n","Epoch 69/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.2648 - acc: 0.9075 - val_loss: 0.3556 - val_acc: 0.8896\n","Epoch 70/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2629 - acc: 0.9077 - val_loss: 0.3474 - val_acc: 0.8906\n","Epoch 71/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2605 - acc: 0.9073 - val_loss: 0.4620 - val_acc: 0.8646\n","Epoch 72/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2627 - acc: 0.9083 - val_loss: 0.3732 - val_acc: 0.8841\n","Epoch 73/150\n","500/500 [==============================] - 29s 59ms/step - loss: 0.2563 - acc: 0.9093 - val_loss: 0.3984 - val_acc: 0.8796\n","Epoch 74/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.2553 - acc: 0.9098 - val_loss: 0.3780 - val_acc: 0.8849\n","Epoch 75/150\n","500/500 [==============================] - 30s 59ms/step - loss: 0.2508 - acc: 0.9128 - val_loss: 0.3878 - val_acc: 0.8849\n","Epoch 76/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2176 - acc: 0.9232 - val_loss: 0.3157 - val_acc: 0.9022\n","Epoch 77/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2053 - acc: 0.9276 - val_loss: 0.3109 - val_acc: 0.9031\n","Epoch 78/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.2003 - acc: 0.9293 - val_loss: 0.3139 - val_acc: 0.9049\n","Epoch 79/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.1974 - acc: 0.9313 - val_loss: 0.3162 - val_acc: 0.9037\n","Epoch 80/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.1937 - acc: 0.9316 - val_loss: 0.3081 - val_acc: 0.9061\n","Epoch 81/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.1939 - acc: 0.9312 - val_loss: 0.3154 - val_acc: 0.9052\n","Epoch 82/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1942 - acc: 0.9315 - val_loss: 0.3133 - val_acc: 0.9048\n","Epoch 83/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.1939 - acc: 0.9308 - val_loss: 0.3128 - val_acc: 0.9042\n","Epoch 84/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1918 - acc: 0.9326 - val_loss: 0.3145 - val_acc: 0.9039\n","Epoch 85/150\n","500/500 [==============================] - 30s 60ms/step - loss: 0.1881 - acc: 0.9335 - val_loss: 0.3144 - val_acc: 0.9052\n","Epoch 86/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.1862 - acc: 0.9336 - val_loss: 0.3162 - val_acc: 0.9052\n","Epoch 87/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1876 - acc: 0.9339 - val_loss: 0.3206 - val_acc: 0.9033\n","Epoch 88/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1828 - acc: 0.9336 - val_loss: 0.3245 - val_acc: 0.9043\n","Epoch 89/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.1839 - acc: 0.9348 - val_loss: 0.3207 - val_acc: 0.9027\n","Epoch 90/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1815 - acc: 0.9352 - val_loss: 0.3116 - val_acc: 0.9062\n","Epoch 91/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.1809 - acc: 0.9354 - val_loss: 0.3111 - val_acc: 0.9062\n","Epoch 92/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1823 - acc: 0.9351 - val_loss: 0.3121 - val_acc: 0.9078\n","Epoch 93/150\n","500/500 [==============================] - 30s 61ms/step - loss: 0.1821 - acc: 0.9351 - val_loss: 0.3138 - val_acc: 0.9064\n","Epoch 94/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1818 - acc: 0.9359 - val_loss: 0.3183 - val_acc: 0.9047\n","Epoch 95/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1815 - acc: 0.9354 - val_loss: 0.3105 - val_acc: 0.9070\n","Epoch 96/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1795 - acc: 0.9367 - val_loss: 0.3157 - val_acc: 0.9063\n","Epoch 97/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1753 - acc: 0.9382 - val_loss: 0.3160 - val_acc: 0.9052\n","Epoch 98/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1777 - acc: 0.9369 - val_loss: 0.3151 - val_acc: 0.9043\n","Epoch 99/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1769 - acc: 0.9363 - val_loss: 0.3187 - val_acc: 0.9037\n","Epoch 100/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1776 - acc: 0.9361 - val_loss: 0.3236 - val_acc: 0.9042\n","Epoch 101/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1742 - acc: 0.9380 - val_loss: 0.3192 - val_acc: 0.9056\n","Epoch 102/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1729 - acc: 0.9371 - val_loss: 0.3227 - val_acc: 0.9055\n","Epoch 103/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1717 - acc: 0.9381 - val_loss: 0.3092 - val_acc: 0.9067\n","Epoch 104/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1707 - acc: 0.9393 - val_loss: 0.3130 - val_acc: 0.9068\n","Epoch 105/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1719 - acc: 0.9384 - val_loss: 0.3214 - val_acc: 0.9050\n","Epoch 106/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1703 - acc: 0.9393 - val_loss: 0.3206 - val_acc: 0.9042\n","Epoch 107/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1742 - acc: 0.9381 - val_loss: 0.3170 - val_acc: 0.9050\n","Epoch 108/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1716 - acc: 0.9396 - val_loss: 0.3145 - val_acc: 0.9070\n","Epoch 109/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1678 - acc: 0.9405 - val_loss: 0.3264 - val_acc: 0.9048\n","Epoch 110/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1717 - acc: 0.9394 - val_loss: 0.3193 - val_acc: 0.9051\n","Epoch 111/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1731 - acc: 0.9382 - val_loss: 0.3167 - val_acc: 0.9055\n","Epoch 112/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1668 - acc: 0.9402 - val_loss: 0.3153 - val_acc: 0.9085\n","Epoch 113/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1689 - acc: 0.9399 - val_loss: 0.3216 - val_acc: 0.9062\n","Epoch 114/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1665 - acc: 0.9409 - val_loss: 0.3200 - val_acc: 0.9077\n","Epoch 115/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1644 - acc: 0.9409 - val_loss: 0.3154 - val_acc: 0.9075\n","Epoch 116/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1665 - acc: 0.9400 - val_loss: 0.3196 - val_acc: 0.9062\n","Epoch 117/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1699 - acc: 0.9397 - val_loss: 0.3241 - val_acc: 0.9043\n","Epoch 118/150\n","500/500 [==============================] - 31s 61ms/step - loss: 0.1641 - acc: 0.9415 - val_loss: 0.3185 - val_acc: 0.9083\n","Epoch 119/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1670 - acc: 0.9403 - val_loss: 0.3261 - val_acc: 0.9054\n","Epoch 120/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1627 - acc: 0.9423 - val_loss: 0.3139 - val_acc: 0.9080\n","Epoch 121/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1688 - acc: 0.9380 - val_loss: 0.3174 - val_acc: 0.9082\n","Epoch 122/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1677 - acc: 0.9399 - val_loss: 0.3296 - val_acc: 0.9065\n","Epoch 123/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1659 - acc: 0.9407 - val_loss: 0.3261 - val_acc: 0.9071\n","Epoch 124/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1647 - acc: 0.9415 - val_loss: 0.3307 - val_acc: 0.9048\n","Epoch 125/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1617 - acc: 0.9427 - val_loss: 0.3201 - val_acc: 0.9071\n","Epoch 126/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1627 - acc: 0.9416 - val_loss: 0.3180 - val_acc: 0.9070\n","Epoch 127/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1629 - acc: 0.9423 - val_loss: 0.3184 - val_acc: 0.9072\n","Epoch 128/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1608 - acc: 0.9421 - val_loss: 0.3168 - val_acc: 0.9069\n","Epoch 129/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1591 - acc: 0.9427 - val_loss: 0.3187 - val_acc: 0.9062\n","Epoch 130/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1585 - acc: 0.9434 - val_loss: 0.3161 - val_acc: 0.9073\n","Epoch 131/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1606 - acc: 0.9429 - val_loss: 0.3174 - val_acc: 0.9070\n","Epoch 132/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1631 - acc: 0.9417 - val_loss: 0.3171 - val_acc: 0.9078\n","Epoch 133/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1587 - acc: 0.9439 - val_loss: 0.3170 - val_acc: 0.9076\n","Epoch 134/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1576 - acc: 0.9442 - val_loss: 0.3167 - val_acc: 0.9073\n","Epoch 135/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1598 - acc: 0.9425 - val_loss: 0.3181 - val_acc: 0.9072\n","Epoch 136/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1582 - acc: 0.9440 - val_loss: 0.3192 - val_acc: 0.9083\n","Epoch 137/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1571 - acc: 0.9440 - val_loss: 0.3183 - val_acc: 0.9078\n","Epoch 138/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1589 - acc: 0.9439 - val_loss: 0.3177 - val_acc: 0.9077\n","Epoch 139/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1580 - acc: 0.9445 - val_loss: 0.3170 - val_acc: 0.9085\n","Epoch 140/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1620 - acc: 0.9417 - val_loss: 0.3182 - val_acc: 0.9079\n","Epoch 141/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1586 - acc: 0.9420 - val_loss: 0.3187 - val_acc: 0.9074\n","Epoch 142/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1564 - acc: 0.9443 - val_loss: 0.3165 - val_acc: 0.9078\n","Epoch 143/150\n","500/500 [==============================] - 32s 63ms/step - loss: 0.1589 - acc: 0.9434 - val_loss: 0.3172 - val_acc: 0.9076\n","Epoch 144/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1584 - acc: 0.9425 - val_loss: 0.3167 - val_acc: 0.9077\n","Epoch 145/150\n","500/500 [==============================] - 31s 62ms/step - loss: 0.1537 - acc: 0.9452 - val_loss: 0.3180 - val_acc: 0.9071\n","Epoch 146/150\n","500/500 [==============================] - 32s 64ms/step - loss: 0.1555 - acc: 0.9442 - val_loss: 0.3160 - val_acc: 0.9076\n","Epoch 147/150\n","500/500 [==============================] - 33s 66ms/step - loss: 0.1563 - acc: 0.9441 - val_loss: 0.3179 - val_acc: 0.9077\n","Epoch 148/150\n","500/500 [==============================] - 32s 64ms/step - loss: 0.1604 - acc: 0.9434 - val_loss: 0.3139 - val_acc: 0.9075\n","Epoch 149/150\n","500/500 [==============================] - 31s 63ms/step - loss: 0.1526 - acc: 0.9449 - val_loss: 0.3157 - val_acc: 0.9071\n","Epoch 150/150\n","500/500 [==============================] - 32s 63ms/step - loss: 0.1594 - acc: 0.9419 - val_loss: 0.3173 - val_acc: 0.9081\n","10000/10000 [==============================] - 1s 140us/step\n"],"name":"stdout"}]}]}