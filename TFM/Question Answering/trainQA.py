# -*- coding: utf-8 -*-
"""trainQA.ipynb

Automatically generated by Colaboratory.

"""

#Comprobamos la GPU que se nos ha asignado
!nvidia-smi

#Montamos nuestro drive en Colab
from google.colab import drive
drive.mount('/content/drive')

#Acceder a la ruta para entrenar
%cd drive/My Drive/Trabajo/TranslateAlignRetrieve/src/qa
!ls

#Instalamos dependencias de python y librerías externas
!pip install -r requirements.txt
!mkdir -p tools

%cd tools
!mkdir transformers
# %cd transformers
!git clone https://github.com/huggingface/transformers.git

%cd ..

#Entrenamos el modelo de Bert multilingual
!pip install transformers==2.8.0
!python ./tools/transformers/examples/run_squad.py --model_type bert --model_name_or_path bert-base-multilingual-cased --do_train --train_file SQuAD-v2.0-eu_small_train.json --save_steps 100000 --predict_file "" --per_gpu_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 384 --doc_stride 128 --version_2_with_negative --overwrite_output_dir --overwrite_cache --output_dir ./data/training/m-bert_train-v2.0-eu.json

#Truco para más RAM (25 GB)
a = []
while(1):
    a.append('1')

#Evaluamos el modelo entrenado de bert con XQuAD
!pip install transformers==2.8.0
!python ./tools/transformers/examples/run_squad.py --model_type bert --model_name_or_path ./data/training/m-bert_train-v2.0-eu.json --train_file ./data/training/m-bert_train-v2.0-eu.json --do_eval --predict_file ./corpora/XQuAD/xquad.eu.json --overwrite_cache --n_best_size 5 --output_dir ./data/training/m-bert_train-v2.0-eu.json
